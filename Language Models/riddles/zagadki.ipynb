{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zagadki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstęp\n",
    "Zagadki od wieków fascynują ludzi, pobudzając ich umysły do kreatywnego i logicznego myślenia. Od prostych łamigłówek po głębokie zagadki filozoficzne, stanowią one nie tylko formę rozrywki, ale również sztukę rozumienia języka i logicznego wnioskowania. W tym zadaniu będziesz rozwiązywał/a zagadki, polegające na odgadnięciu słowa na podstawie opisu. Wszystkie zagadki wymyślił ChatGPT (ale nie powiemy, która dokładnie wersja i w jaki sposób zachęcona), stąd niektóre mogą być trochę dziwne... Szacujemy, że ludzie są w stanie rozwiązać poprawnie trochę ponad 60% z nich. A jak dobry będzie Twój program?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie\n",
    "Napisz funckję `answer_riddle` która rozwiąże podaną na wejściu zagadkę. Rozwiązaniem jest zawsze jedno słowo. Przykładowe zagadki:\n",
    "\n",
    "- **zagadka:** kobieta podróżująca środkiem transportu, np. samolotem, pociągiem, statkiem <br>\n",
    "  **odpowiedź:** pasażerka\n",
    "- **zagadka:** emocjonalne uczucie łączące dwie osoby, oparte na zaufaniu, szacunku, trosce i oddaniu<br>\n",
    "  **odpowiedź:** miłość\n",
    "\n",
    "\n",
    "Naszym kryterium będzie `odwrócona średnia harmoniczna` ([Mean Reciprocal Rank](https://en.wikipedia.org/wiki/Mean_reciprocal_rank)), która działa w następujący sposób: <br>\n",
    "Jeżeli na zwróconej przez Twoją funkcję liście znajdzie się prawidłowa odpowiedź, otrzymasz wówczas punkty: dokładnie $\\frac{1}{k}$ punktów, gdzie $k$ jest pozycją słowa na liście. W szczególności, jeżeli Twój program zgadnie słowo (czyli umieści je na pierwszej pozycji), to otrzymasz 1 punkt. Ostatecznym kryterium jest uśredniona liczba punktów ze wszystkich zagadek.\n",
    "\n",
    "Powyższe kryterium jest zaimplementowane poniżej przez nas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ograniczenia\n",
    "- Twoje finalne rozwiązanie będzie testowane w środowisku **bez** GPU.\n",
    "- Twoja funkcja powinna działać na tyle szybko, aby program był w stanie udzielić odpowiedzi na 100 zagadek w maksymalnie 2 minuty bez użycia GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dane\n",
    "Dane dostępne dla Ciebie w tym zadaniu to:\n",
    "* `zagadki_do_testow_clean.txt` - około 2000 przykładowych zagadek\n",
    "* `plwiktionary_definitions_clean.txt` -  plik z definicjami słów wziętymi z [pl.wiktionary.org](https://pl.wiktionary.org/wiki/pl). Z wszystkich definicji z pl.wiktionary.org wzięliśmy definicje 8094 najbardziej popularnych rzeczowników (częstości liczone zgodnie z korpusem https://2018.poleval.pl/index.php/tasks#task3). Uwaga: poprawne rozwiązanie każdej zagadki znajduje się w tym pliku!\n",
    "\n",
    "* `superbazy_clean.txt` - formy bazowe polskich słów, przygotowane na podstawie projektu https://github.com/morfologik/polimorfologik\n",
    "\n",
    "* Wektory osadzeń słów bazowych, wytrenowane modelem Word2Vec z biblioteki Gensim, na korpusie PolEval 2018 Task3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uwagi i wskazówki\n",
    "- Dla każdej zagadki, Twoja funkcja powinna zwrócić listę słów (co najwyżej 20), w kolejności od najbardziej (wg Twojego programu) prawdopodobnej odpowiedzi na zagadkę, do najmniej.\n",
    "- Twoje rozwiazanie bedzie testowane bez dostepu do internetu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pliki Zgłoszeniowe\n",
    "Tylko ten notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ewaluacja\n",
    "Pamiętaj, że podczas sprawdzania flaga `FINAL_EVALUATION_MODE` zostanie ustawiona na `True`. Za pomocą skryptu `validation_script.py` będziesz upewnić się, że Twoje rozwiązanie zostanie prawidłowo wykonane na naszych serwerach oceniających. \n",
    "\n",
    "Za to zadanie możesz zdobyć pomiędzy 0 i 1.5 punktu. Zdobędziesz 0 punktów jeśli wartość kryterium `mean reciprocal rank` na zbiorze testowym wyniesie poniżej 0.02, a 1.5 punktu jeśli wyniesie powyżej 0.3. Pomiędzy tymi wartościami, wynik rośnie liniowo z wartością kryterium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kod startowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI PODCZAS WYSYŁANIA ##########################\n",
    "FINAL_EVALUATION_MODE = False\n",
    "# W czasie sprawdzania Twojego rozwiązania, zmienimy tę wartość na True\n",
    "# Wartość tej flagi M U S I zostać ustawiona na False w rozwiązaniu, które nam nadeślesz!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize as tokenize\n",
    "from collections import defaultdict as dd\n",
    "import math\n",
    "from gensim.models import Word2Vec\n",
    "import gdown\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.nn import functional as F\n",
    "from transformers import logging\n",
    "from pandas import Series\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.set_verbosity_error()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ładowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = './data/'\n",
    "\n",
    "bases = {}\n",
    "# Dictionary mapping words to their base words\n",
    "all_word_definitions = dd(list)\n",
    "# Dictionary containing all base words inverse document frequency\n",
    "base_idf = dd(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_base(word):\n",
    "    global bases\n",
    "    word = word.lower()\n",
    "    ret = bases.get(word)\n",
    "    if ret:\n",
    "        return ret\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/patryk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "if not FINAL_EVALUATION_MODE:\n",
    "    if not os.path.exists(f\"{path_to_data}/zagadki/w2v_polish_lemmas.model\") \\\n",
    "        or not os.path.exists(f\"{path_to_data}/zagadki/w2v_polish_lemmas.model.syn1neg.npy\") \\\n",
    "        or not os.path.exists(f\"{path_to_data}/zagadki/w2v_polish_lemmas.model.wv.vectors.npy\"):\n",
    "            gdown.download_folder(url=\"https://drive.google.com/drive/folders/1P72og_ORfL3Ojf27n-g06DT0ENduPy8C?usp=sharing\", output=f\"./{path_to_data}\")\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tworzenie bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in open(f'{path_to_data}/zagadki/superbazy_clean.txt'):\n",
    "    word,base = x.lower().split()\n",
    "    bases[word] = base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ladowanie modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec = Word2Vec.load(f'{path_to_data}/zagadki/w2v_polish_lemmas.model')\n",
    "\n",
    "model_name_pauga = 'flax-community/papuGaPT2'\n",
    "tokenizer_papuga = AutoTokenizer.from_pretrained(model_name_pauga)\n",
    "model_papuga = AutoModelForCausalLM.from_pretrained(model_name_pauga).to(device)\n",
    "emb_papuga = model_papuga.transformer.wte.weight.detach().cpu().numpy()\n",
    "del model_papuga\n",
    "\n",
    "model_name_polka = 'eryk-mazus/polka-1.1b'\n",
    "tokenizer_polka = AutoTokenizer.from_pretrained(model_name_polka)\n",
    "model_polka = AutoModelForCausalLM.from_pretrained(model_name_polka).to(device)\n",
    "\n",
    "model_name_herbert = \"allegro/herbert-base-cased\"\n",
    "tokenizer_herbert = AutoTokenizer.from_pretrained(model_name_herbert)\n",
    "model_herbert = AutoModel.from_pretrained(model_name_herbert).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tworzenie answers i queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "queries = []\n",
    "\n",
    "with open(f'{path_to_data}/zagadki/zagadki_do_testow_clean.txt') as file:\n",
    "    for line in file:\n",
    "        line = line.replace(';;', '').split()                  \n",
    "        answers.append(line[0])\n",
    "        queries.append(' '.join(line[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rękopiśmienny tekst lub dokument, niepublikowany drukiem.', 'manuskrypt')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[0], answers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tworzenie all_word_definitions, base_idf, query, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1700/1700 [00:00<00:00, 38331.74it/s]\n"
     ]
    }
   ],
   "source": [
    "punctuation = {'!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '–', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '``', \"''\"}\n",
    "\n",
    "for x in open(f'{path_to_data}/zagadki/plwiktionary_definitions_clean.txt'):\n",
    "    word, definition = x.split('###')\n",
    "    L = word.split()\n",
    "    if len(L) == 1:\n",
    "        word = L[0]\n",
    "        definition = definition.replace('\\n', '')\n",
    "        definition = definition.replace('|', ' ')\n",
    "        definition = ''.join(c for c in definition.lower() if c not in punctuation)\n",
    "        if len(definition) == 0: continue\n",
    "        all_word_definitions[word].append(definition)\n",
    "        for word in set(definition.split()):\n",
    "            base_idf[get_word_base(word)] += 1\n",
    "\n",
    "for query, answer in tqdm(zip(queries[293:], answers[293:]), total=len(queries[293:])):\n",
    "    all_word_definitions[answer].append(''.join(c for c in query if c not in punctuation))\n",
    "    for word in set(query.split()):\n",
    "        base_idf[get_word_base(word)] += 1\n",
    "\n",
    "for base in base_idf:\n",
    "    base_idf[base] = -math.log(base_idf[base] / len(all_word_definitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    21982.000000\n",
       "mean         8.175645\n",
       "std          1.116514\n",
       "min          0.611736\n",
       "25%          7.611842\n",
       "50%          8.998137\n",
       "75%          8.998137\n",
       "max          8.998137\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Series(base_idf).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia 1.2066139415495751\n",
      "w 0.6117358595340934\n",
      "on 2.4216671916520833\n",
      "o 2.1427279620903796\n",
      "lub_lubić 0.7472556159996584\n",
      "od 2.4849066497880004\n",
      "z 1.1955186972576357\n",
      "do 1.2049624135111021\n",
      "coś 1.680260562073812\n",
      "jakiś 2.4470564256569025\n",
      "osoba 1.9824243402130781\n",
      "przen 2.2924976658403047\n",
      "i 1.2540001330723172\n",
      "który 1.5336269260637798\n",
      "pot 1.9743778059618642\n",
      "się 1.4333797477945782\n",
      "na 1.2830131570682022\n",
      "przez 2.492352700572079\n",
      "ten 2.4599969369326375\n"
     ]
    }
   ],
   "source": [
    "for w, v in base_idf.items():\n",
    "    if v < 2.5: print(w, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' geogr hydrol duży zbiornik słonej wody mający połączenie z oceanem wikipedia',\n",
       " ' przen ogromna ilość czegoś',\n",
       " 'duże słone zbiornik wodny połączone z oceanem zazwyczaj otoczone lądem']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_word_definitions['morze']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kod z kryteriami oceniającymi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(real_answers, computed_answers, K=20):\n",
    "    positions = []\n",
    "\n",
    "    for real_answer, computed_answer in zip(real_answers, computed_answers):\n",
    "        if real_answer in computed_answer[:K]:\n",
    "            pos = computed_answer.index(real_answer) + 1\n",
    "            positions.append(1/pos)\n",
    "    \n",
    "    mrr = sum(positions) / len(real_answers)\n",
    "    print ('Mean Reciprocal Rank =', mrr)\n",
    "    \n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twoje rozwiązanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To jest jedyna sekcja, w której musisz coś zrobić."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(a, b, epsilon=1e-10):\n",
    "    return a.dot(b) / ((a.dot(a) * b.dot(b)) ** 0.5 + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAPUGA & IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_embedding(word):\n",
    "    token_ids = tokenizer_papuga.encode(' ' + word)\n",
    "    # emb_papuga[token_ids[0]] *= 3.0\n",
    "    # whole_word_embbeding = np.sum(np.stack([emb_papuga[id] for id in token_ids]), axis=0)\n",
    "    whole_word_embbeding = np.sum(np.stack([emb_papuga[id] for id in token_ids]), axis=0) / len(token_ids)\n",
    "    return whole_word_embbeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8090/8090 [02:11<00:00, 61.64it/s] \n"
     ]
    }
   ],
   "source": [
    "all_word_definitions_papuga_embeddings = dd(list)\n",
    "\n",
    "for word in tqdm(all_word_definitions, total=len(all_word_definitions)):\n",
    "    for definition in all_word_definitions[word]:\n",
    "        definition = list(definition)\n",
    "        first_w_in_definition = definition[0]\n",
    "        base = get_word_base(first_w_in_definition)\n",
    "        res_embbeding = decoder_embedding(first_w_in_definition) * -base_idf[get_word_base(base)]\n",
    "        # res_embbeding = decoder_embedding(first_w_in_definition)\n",
    "        for w in definition[1:]:\n",
    "            base = get_word_base(w)\n",
    "            res_embbeding += decoder_embedding(w) * -base_idf[base]\n",
    "            # res_embbeding += decoder_embedding(w)\n",
    "        all_word_definitions_papuga_embeddings[word].append(res_embbeding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def papuga_idf_answer_riddle_word_score(riddle, K):\n",
    "    # papuga & idf & earest neighbor word2vec\n",
    "    riddle = list(filter(lambda x: x not in punctuation, riddle))\n",
    "    res_embbeding = decoder_embedding(riddle[0]) * -base_idf[get_word_base(riddle[0])]\n",
    "    # res_embbeding = decoder_embedding(riddle[0])\n",
    "    for word in riddle[1:]:\n",
    "        res_embbeding += decoder_embedding(word) * -base_idf[get_word_base(word)]\n",
    "        # res_embbeding += decoder_embedding(word)\n",
    "    \n",
    "    closest_words = []\n",
    "    for word, embeddings in all_word_definitions_papuga_embeddings.items():\n",
    "        for emb in embeddings:\n",
    "            # distance = np.linalg.norm(res_embbeding - emb)\n",
    "            distance = cos(res_embbeding, emb)\n",
    "            closest_words.append((distance, word))\n",
    "    \n",
    "    closest_words = sorted(closest_words, key=lambda x: x[0])\n",
    "    return closest_words[-K:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HERTBER & IDF ( aktualnie bez idf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_embedding(L):\n",
    "    txt = ' '.join(L)\n",
    "    input_ids = tokenizer_herbert(txt, return_tensors='pt')['input_ids'].to(device)\n",
    "    output = model_herbert(input_ids=input_ids)\n",
    "    return output.last_hidden_state.detach().cpu().numpy()[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8088/8088 [06:01<00:00, 22.38it/s]\n"
     ]
    }
   ],
   "source": [
    "all_word_definitions_herbert_embeddings = dd(list)\n",
    "\n",
    "for word in tqdm(all_word_definitions):\n",
    "    for definition in all_word_definitions[word]:\n",
    "        def_embbeding = encoder_embedding(definition)\n",
    "        all_word_definitions_herbert_embeddings[word].append(def_embbeding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def herbert_idf_answer_riddle_word_score(riddle, K):\n",
    "    # riddle = list(filter(lambda x: x not in punctuation and base_idf[x] > 2.5, riddle))\n",
    "    riddle = list(filter(lambda x: x not in punctuation, riddle))\n",
    "    riddle_embbeding = encoder_embedding(riddle)\n",
    "\n",
    "    closest_words = []\n",
    "    for word, embeddings in all_word_definitions_herbert_embeddings.items():\n",
    "        # idf = base_idf[get_word_base(word)]\n",
    "        for emb in embeddings:\n",
    "            # distance = np.linalg.norm(res_embbeding - emb)\n",
    "            distance = cos(riddle_embbeding, emb)\n",
    "            # distance = idf * cos(riddle_embbeding, emb)\n",
    "            closest_words.append((distance, word))\n",
    "    \n",
    "    closest_words = sorted(closest_words, key=lambda x: x[0])\n",
    "    return [ x[1] for x in closest_words[-K:]]\n",
    "    # return closest_words[-K:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD2VEC & IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "def cos_np(a, b):\n",
    "    if a.shape != b.shape:\n",
    "        print(a.shape, b.shape)\n",
    "    assert a.shape == b.shape\n",
    "    return dot(a, b)/(norm(a)*norm(b) + np.finfo(float).eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_embedding(word):\n",
    "    try:\n",
    "        return model_word2vec.wv[word]\n",
    "    except:\n",
    "        return np.zeros(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8088/8088 [00:02<00:00, 3211.51it/s]\n"
     ]
    }
   ],
   "source": [
    "all_word_definitions_wrd2vec_embeddings = dd(list)\n",
    "\n",
    "for word in tqdm(all_word_definitions):\n",
    "    for definition in all_word_definitions[word]:\n",
    "        definition_tekenized = tokenize(definition)\n",
    "        def_embbeding = np.sum([word2vec_embedding(get_word_base(w)) for w in definition_tekenized if w not in punctuation and base_idf[get_word_base(w)] > 1.9], axis=0)\n",
    "        if not isinstance(def_embbeding, np.ndarray): continue\n",
    "        all_word_definitions_wrd2vec_embeddings[word].append(def_embbeding)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_idf_answer_riddle_word_score(riddle, K):\n",
    "    riddle_embbeding = np.sum([word2vec_embedding(get_word_base(w)) for w in riddle if base_idf[get_word_base(w)] > 1.9], axis=0)\n",
    "\n",
    "    closest_words = []\n",
    "    for word, embeddings in all_word_definitions_wrd2vec_embeddings.items():\n",
    "        for emb in embeddings:\n",
    "            distance = cos_np(riddle_embbeding, emb)\n",
    "            closest_words.append((distance, word))\n",
    "    \n",
    "    closest_words = sorted(closest_words, key=lambda x: x[0])\n",
    "    return closest_words[-K:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLKA & filtrowanie generacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Przeanalizuj podane zdanie i odpowiedz na nie jednym słowem, które najlepiej pasuje do treści. Twoja odpowiedź powinna zawierać tylko jeden wyraz.\n",
    "\n",
    "Przykłady:\n",
    "\n",
    "Zagadka: mały owad produkujący miód.\n",
    "Odpowiedź: pszczoła\n",
    "\n",
    "Zagadka: drapieżny ssak, uważany za króla dżungli.\n",
    "Odpowiedź: lew\n",
    "\n",
    "Zagadka: osoba, która pisze książki.\n",
    "Odpowiedź: pisarz\n",
    "\n",
    "Do rozwiązania:\n",
    "Zagadka: [[zagadka]]\n",
    "Odpowiedź:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polka_answer_riddle(riddle, K):\n",
    "    prompt_filled = prompt.replace('[[zagadka]]', riddle)\n",
    "    input_ids = tokenizer_polka(prompt_filled, return_tensors='pt').input_ids.to(device)\n",
    "    output = model_polka.generate(input_ids, max_length=300, num_return_sequences=K, do_sample=True, top_k=K)\n",
    "    answers = [tokenizer_polka.decode(output[i], skip_special_tokens=True).split('Odpowiedź:')[-1].strip().split()[0] for i in range(K)]\n",
    "    answers = list(filter(lambda x: x not in all_word_definitions.keys(), answers))\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Training accuracy: 0.7633079686090257\n",
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for query, answer in zip(queries[293:], answers[293:]):\n",
    "    query_embedding = decoder_embedding(query) # PAPUGA\n",
    "    # query_embedding = encoder_embedding(query) # HERBERT\n",
    "    num_definitions = len(all_word_definitions[answer])\n",
    "    negative_samples = []\n",
    "\n",
    "    while len(negative_samples) < num_definitions * 2:\n",
    "        # tutja jeszcze powinien byc set w licie aby bylo bez powtorzen\n",
    "        negative_samples = random.sample(list(all_word_definitions_papuga_embeddings.keys()), num_definitions * 2)\n",
    "        # negative_samples = random.sample(list(all_word_definitions_herbert_embeddings.keys()), num_definitions * 2)\n",
    "    \n",
    "    for emb in all_word_definitions_papuga_embeddings[answer]: # PAPUGA\n",
    "    # for emb in all_word_definitions_herbert_embeddings[answer]: # HERBERT\n",
    "        combined_embedding = np.concatenate((query_embedding, emb))\n",
    "        for i in range(2): # żeby było tyle samo positive samples i negative samples\n",
    "            X.append(combined_embedding)\n",
    "            y.append(1)  # correct answer\n",
    "\n",
    "    for word in negative_samples:\n",
    "        for emb in all_word_definitions_papuga_embeddings[word]: # PAPUGA\n",
    "        # for emb in all_word_definitions_herbert_embeddings[word]: # HERBERT\n",
    "            combined_embedding = np.concatenate((query_embedding, emb))\n",
    "            X.append(combined_embedding)\n",
    "            y.append(0)  # incorrect answer\n",
    "\n",
    "print(1)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = X[:1700], X[1700:], y[:1700], y[1700:]\n",
    "X_train = np.array(X)\n",
    "y_train = np.array(y)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for query, answer in zip(queries[:293], answers[:293]):\n",
    "    query_embedding = decoder_embedding(query) # PAPUGA\n",
    "    # query_embedding = encoder_embedding(query) # HERBERT\n",
    "    \n",
    "    for emb in all_word_definitions_papuga_embeddings[answer]: # PAPUGA\n",
    "    # for emb in all_word_definitions_herbert_embeddings[answer]: # HERBERT\n",
    "        combined_embedding = np.concatenate((query_embedding, emb))\n",
    "        X.append(combined_embedding)\n",
    "        y.append(1)  # correct answer\n",
    "\n",
    "print(2)\n",
    "\n",
    "X_test = np.array(X)\n",
    "y_test = np.array(y)\n",
    "\n",
    "model = LogisticRegression(max_iter=10000, C=0.1)  # Adding L2 regularization with C=0.1\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(3)\n",
    "\n",
    "train_score = model.score(X_train, y_train)\n",
    "\n",
    "print(4)\n",
    "\n",
    "val_score = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training accuracy: {train_score}\")\n",
    "print(f\"Test accuracy: {val_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_answer_riddle(riddle, K):\n",
    "    riddle_embedding = encoder_embedding(riddle)\n",
    "    scores = []\n",
    "    for word, embeddings in all_word_definitions_herbert_embeddings.items():\n",
    "        for emb in embeddings:\n",
    "            combined_embedding = np.concatenate((riddle_embedding, emb))\n",
    "            score = model.predict_proba([combined_embedding])[0][1]  # probability of being the correct answer\n",
    "            scores.append((score, word))\n",
    "    \n",
    "    scores = sorted(scores, key=lambda x: x[0], reverse=True)\n",
    "    return [word for _, word in scores[:K]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rękopiśmienny tekst lub dokument, niepublikowany drukiem.', 'manuskrypt')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[0], answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lampa',\n",
       " 'łoże',\n",
       " 'tur',\n",
       " 'as',\n",
       " 'por',\n",
       " 'contra',\n",
       " 'om',\n",
       " 'om',\n",
       " 'pan',\n",
       " 'gros',\n",
       " 'zapowiedź',\n",
       " 'limit',\n",
       " 'modus',\n",
       " 'ciemny',\n",
       " 'cola',\n",
       " 'focus',\n",
       " 'kres',\n",
       " 'schemat',\n",
       " 'ograniczenie',\n",
       " 'agent']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_answer_riddle(queries[0], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### puting all togethr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_riddle():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ewaluacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższy kod będzie służył ewaluacji rozwiązania. Po wysłaniu rozwiązania do nas zostanie wykonana funkcja `evaluate_algorithm(score_function, queries, answers, K)`, t.j. prawie identyczny kod jak niżej będzie się uruchamiał na katalogu danych `test_data` dostępnym tylko dla sprawdzających zadania.\n",
    "\n",
    "Upewnij się przed wysłaniem, że cały notebook wykonuje się od początku do końca bez błędów i bez ingerencji użytkownika po wykonaniu polecenia `Run All`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "def evaluate_algorithm(score_function, queries, answers, K):\n",
    "    computed_answers = []\n",
    "    for query in tqdm(queries, desc=\"queries answered\"):\n",
    "        computed_answers.append(score_function(query, K=K))\n",
    "    score = mean_reciprocal_rank(answers, computed_answers, K=K)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "queries answered:   1%|▏         | 4/293 [00:11<13:14,  2.75s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m valid_queries \u001b[38;5;241m=\u001b[39m queries[:PART_OF_DATA]\n\u001b[1;32m      6\u001b[0m valid_answers \u001b[38;5;241m=\u001b[39m answers[:PART_OF_DATA]\n\u001b[0;32m----> 7\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolka_answer_riddle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_answers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[47], line 5\u001b[0m, in \u001b[0;36mevaluate_algorithm\u001b[0;34m(score_function, queries, answers, K)\u001b[0m\n\u001b[1;32m      3\u001b[0m computed_answers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m tqdm(queries, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueries answered\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     computed_answers\u001b[38;5;241m.\u001b[39mappend(\u001b[43mscore_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m score \u001b[38;5;241m=\u001b[39m mean_reciprocal_rank(answers, computed_answers, K\u001b[38;5;241m=\u001b[39mK)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "Cell \u001b[0;32mIn[74], line 4\u001b[0m, in \u001b[0;36mpolka_answer_riddle\u001b[0;34m(riddle, K)\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt_filled \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[[zagadka]]\u001b[39m\u001b[38;5;124m'\u001b[39m, riddle)\n\u001b[1;32m      3\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer_polka(prompt_filled, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_polka\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m answers \u001b[38;5;241m=\u001b[39m [tokenizer_polka\u001b[38;5;241m.\u001b[39mdecode(output[i], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOdpowiedź:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K)]\n\u001b[1;32m      6\u001b[0m answers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_word_definitions\u001b[38;5;241m.\u001b[39mkeys(), answers))\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2245\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2246\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2247\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2249\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2265\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2266\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2272\u001b[0m     )\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/transformers/generation/utils.py:3220\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3217\u001b[0m         model_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_compiled_call(generation_config\u001b[38;5;241m.\u001b[39mcompile_config)\n\u001b[1;32m   3219\u001b[0m is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3220\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(\n\u001b[1;32m   3221\u001b[0m     this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice, cur_len\u001b[38;5;241m=\u001b[39mcur_len, max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[1;32m   3222\u001b[0m ):\n\u001b[1;32m   3223\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3224\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3226\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    PART_OF_DATA = 293\n",
    "    K = 20\n",
    "    valid_queries = queries[:PART_OF_DATA]\n",
    "    valid_answers = answers[:PART_OF_DATA]\n",
    "    score = evaluate_algorithm(polka_answer_riddle, valid_queries, valid_answers, K=K)\n",
    "    print(f\"Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " herbert \\\n",
    "queries answered: 100%|██████████| 293/293 [01:09<00:00,  4.19it/s] \\\n",
    "Mean Reciprocal Rank = 0.002384157962328695 \\\n",
    "Score: 0.002384157962328695"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ml \\\n",
    "queries answered: 100%|██████████| 293/293 [17:14<00:00,  3.53s/it] \\\n",
    "Mean Reciprocal Rank = 0.0005281976271737364 \\\n",
    "Score: 0.0005281976271737364"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec idf \\\n",
    "queries answered: 100%|██████████| 293/293 [02:03<00:00,  2.38it/s] \\\n",
    "Mean Reciprocal Rank = 0.0 \\\n",
    "Score: 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "papuga suma ebedow slow wazona  -idf \\\n",
    "queries answered: 100%|██████████| 293/293 [01:01<00:00,  4.74it/s] \\\n",
    "Mean Reciprocal Rank = 0.020161534304620136 \\\n",
    "Score: 0.020161534304620136\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
